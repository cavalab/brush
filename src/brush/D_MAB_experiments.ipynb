{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing D-MAB, as described in DaCosta et al. - 2008 - Adaptive operator selection with dynamic multi-arm**\n",
    "\n",
    ">  (hybrid between UCB1 and Page-Hinkley (PH) test)\n",
    "\n",
    "D-MAB maintains four indicators for each arm $i$:\n",
    "1. number $n_{i, t}$ of times $i$-th arm has been played up to time $t$;\n",
    "2. the average empirical reward $\\widehat{p}_{j, t}$ at time $t$;\n",
    "3. the average and maximum deviation $m_i$ and $M_i$ involved in the PH test, initialized to $0$ and updated as detailed below. At each time step $t$:\n",
    "\n",
    "D-MAB selects the arm $i$ that maximizes equation 1:\n",
    "\n",
    "$$\\widehat{p}_{i, t} + \\sqrt{\\frac{2 \\log \\sum_{k}n_{k, t}}{n_{i, t}}}$$\n",
    "\n",
    "> Notice that the sum of the number of times each arm was pulled is equal to the time $\\sum_{k}n_{k, t} = t$, but since their algorithm resets the number of picks, we need to go with the summation. \n",
    "\n",
    "and receives some reward $r_t$, drawn after reward distribution $p_{i, t}$.\n",
    "\n",
    "> I think there is a typo in the eq. 1 on the paper. I replaced $j$ with $i$ in the lower indexes.\n",
    "\n",
    "The four indicators are updated accordingly:\n",
    "\n",
    "- $\\widehat{p}_{i, t} :=\\frac{1}{n_{i, t} + 1}(n_{i, t}\\widehat{p}_{i, t} + r_t)$\n",
    "- $n_{i, t} := n_{i, t}+1$\n",
    "- $m_i := m_i + (\\widehat{p}_{i, t} - r_t + \\delta)$\n",
    "- $M_i:= \\text{max}(M_i, m_i)$\n",
    "\n",
    "And if the PH test is triggered ($M_i - m_i > \\lambda$), the bandit is restarted, i.e., for all arms, all indicators are set to zero (the authors argue that, empirically, resetting the values is more robust than decreasing them with some mechanism such as probability matching).\n",
    "\n",
    "> I will reset to 1 instead of 0 (as the original paper does) to avoid divide by zero when calculating UCB1.\n",
    "\n",
    "The PH test is a standard test for the change hypothesis. It works by monitoring the difference between $M_i$ and $m_i$, and when the difference is greater than some uuser-specified threshold $\\lambda$, the PH test is triggered, i.e., it is considered that the Change hypothesis holds.\n",
    "\n",
    "Parameter $\\lambda$ controls the trade-off between false alarms and un-noticed changes. Parameter $\\delta$ enforces the robustness of the test when dealing with slowly varying environments.\n",
    "\n",
    "We also need a scaling mechanism to control the Exploration _versus_ Exploitation balance. They proposed two, from which I will focus on the first: Multiplicative Scaling (cUCB). **It consists on multiplying all rewards by a fixed user-defined parameter $C_{M-\\text{scale}}$.\n",
    "\n",
    "This way, we need to give to our D-MAB 3 parameters: $\\lambda$, $\\delta$, and $C_{M - \\text{scale}}$. In the paper they did a sensitivity analysis of the parameters, but I think they should be fine tuned for each specific data set.\n",
    "\n",
    "> Brush originally sample the mutations using an uniform distribution. This algorithm chooses the arms using an deterministic approach --- the one that maximizes the UCB1 score. Somehow we need to convert them to have a transparent implementation to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib > /dev/null\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from brush.estimator import BrushEstimator\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "from deap import creator\n",
    "import _brush\n",
    "from deap_api import nsga2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_MAB:\n",
    "    def __init__(self, num_bandits, delta=0.15, lmbda=0.25):\n",
    "        self.num_bandits = num_bandits\n",
    "\n",
    "        # Store learner status when the update function is called\n",
    "        self.pull_history = {\n",
    "            c:[] for c in ['t', 'arm idx', 'reward', 'update'] + \n",
    "                          [f'UCB1 {i}'  for i in range(num_bandits)] + \n",
    "                          [f'weight {i}' for i in range(num_bandits)] } \n",
    "\n",
    "        # This is the probability that should be used to update brush probs\n",
    "        self._probabilities = np.ones(num_bandits)/num_bandits\n",
    "\n",
    "        self.delta = delta # how to define these values???\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "        self._reset_indicators() # Creating the indicators \n",
    "\n",
    "    def _reset_indicators(self):\n",
    "        self._avg_rewards    = np.zeros(self.num_bandits)\n",
    "        self._num_pulls      = np.zeros(self.num_bandits)\n",
    "        self._avg_deviations = np.zeros(self.num_bandits)\n",
    "        self._max_deviations = np.zeros(self.num_bandits)\n",
    "\n",
    "    def _calculate_UCB1s(self):\n",
    "        # We need that the reward is in [0, 1] (not avg_reward, as it seems to\n",
    "        # render worse results). It looks like normalizing the rewards is a\n",
    "        # problem: reward should be [0, 1], but not necessarely avg_rewards too\n",
    "        rs = self._avg_rewards\n",
    "        ns = self._num_pulls\n",
    "        \n",
    "        UCB1s = rs + np.sqrt(2*np.log1p(sum(ns))/(ns+1))\n",
    "\n",
    "        return UCB1s\n",
    "\n",
    "    @property\n",
    "    def probabilities(self):\n",
    "        # How to transform our UCB1 scores into node probabilities?\n",
    "        return self._probabilities\n",
    "    \n",
    "    @probabilities.setter\n",
    "    def probabilities(self, new_probabilities):\n",
    "        if len(self._probabilities)==len(new_probabilities):\n",
    "            self._probabilities = new_probabilities\n",
    "        else:\n",
    "            print(f\"New probabilities must have size {self.num_bandits}\")\n",
    "\n",
    "    def choose_arm(self):\n",
    "        \"\"\"Uses previous recordings of rewards to pick the arm that maximizes\n",
    "        the UCB1 function. The choice is made in a deterministic way.\n",
    "        \"\"\"\n",
    "\n",
    "        UCB1s = self._calculate_UCB1s()\n",
    "\n",
    "        return np.nanargmax(UCB1s)\n",
    "\n",
    "    def update(self, arm_idx, reward):\n",
    "        # Here we expect that the reward was already scaled to be in the \n",
    "        # interval [0, 1] (in the original paper, they sugest using a scaling\n",
    "        # factor as an hyperparameter).\n",
    "        self.pull_history['t'].append( len(self.pull_history['t']) )\n",
    "        self.pull_history['arm idx'].append( arm_idx )\n",
    "        self.pull_history['reward'].append( reward )\n",
    "\n",
    "        # Updating counters\n",
    "        self._avg_rewards[arm_idx]    = \\\n",
    "            (self._num_pulls[arm_idx]*self._avg_rewards[arm_idx] + reward)/(self._num_pulls[arm_idx]+1)\n",
    "        self._avg_deviations[arm_idx] = \\\n",
    "            self._avg_deviations[arm_idx] + (self._avg_rewards[arm_idx] - reward + self.delta)    \n",
    "        self._num_pulls[arm_idx]    = self._num_pulls[arm_idx] +1\n",
    "        self._max_deviations[arm_idx] = \\\n",
    "            np.maximum(self._max_deviations[arm_idx], self._avg_deviations[arm_idx])\n",
    "\n",
    "        if (self._max_deviations[arm_idx] - self._avg_deviations[arm_idx] > self.lmbda):\n",
    "            self._reset_indicators()\n",
    "            self.pull_history['update'].append( 1 )\n",
    "        else:\n",
    "            self.pull_history['update'].append( 0 )\n",
    "\n",
    "        self._probabilities = self._calculate_UCB1s()\n",
    "\n",
    "        for i, UCB1 in enumerate(self._calculate_UCB1s()):\n",
    "            self.pull_history[f'UCB1 {i}'].append( UCB1 )\n",
    "            self.pull_history[f'weight {i}'].append( self.probabilities[i] )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learner_history(learner, arm_labels=[]):\n",
    "\n",
    "    # getting the labels to use in plots\n",
    "    if len(arm_labels) != learner.num_bandits:\n",
    "        arm_labels = [f'arm {i}' for i in range(learner.num_bandits)]\n",
    "\n",
    "    # Setting up the figure layout\n",
    "    fig = plt.figure(figsize=(15, 10), tight_layout=True)\n",
    "    gs = gridspec.GridSpec(7, 6)\n",
    "\n",
    "    learner_log = pd.DataFrame(learner.pull_history).set_index('t')\n",
    "                \n",
    "    total_rewards = learner_log.groupby('arm idx')['reward'].sum().to_dict()\n",
    "    total_pulls   = learner_log['arm idx'].value_counts().to_dict()\n",
    "\n",
    "    data_total_pulls    = np.array([total_pulls[k] for k in sorted(total_pulls)])\n",
    "    data_total_rewards  = np.array([total_rewards[k] for k in sorted(total_rewards)])\n",
    "    data_total_failures = data_total_pulls-data_total_rewards\n",
    "\n",
    "    ylim = np.maximum(data_total_rewards.max(), data_total_failures.max())\n",
    "\n",
    "    axs = fig.add_subplot(gs[0:2, 4:])\n",
    "\n",
    "    axs.bar(arm_labels, -1*data_total_failures, label=\"Null reward\")\n",
    "    axs.bar(arm_labels, data_total_rewards, label=\"Positive reward\")\n",
    "\n",
    "    axs.set_xlabel(\"Arm\")\n",
    "    axs.set_ylim( (-1.05*ylim, 1.05*ylim) )\n",
    "    axs.legend()\n",
    "\n",
    "    win_ratios = pd.DataFrame.from_dict({\n",
    "        'arm'      : arm_labels,\n",
    "        'totpulls' : data_total_pulls,\n",
    "        '0 reward' : data_total_failures,\n",
    "        '+ reward' : data_total_rewards,\n",
    "        'success%' : (data_total_rewards/(data_total_pulls)).round(2)\n",
    "    })\n",
    "\n",
    "    axs = fig.add_subplot(gs[2:4, 4:])\n",
    "    axs.table(cellText=win_ratios.values, colLabels=win_ratios.columns, loc='center')\n",
    "    axs.axis('off')\n",
    "    axs.axis('tight')\n",
    "\n",
    "    # Plotting rewards and pulls -----------------------------------------------\n",
    "    # plot the cumulative number of pulls (for evaluations, not generations) ---\n",
    "    data = np.zeros( (learner_log.shape[0]+1, 4) )\n",
    "    for i, row in learner_log.iterrows():\n",
    "        data[i+1, :] = data[i]\n",
    "        data[i+1, row['arm idx'].astype(int)] += 1\n",
    "\n",
    "    axs = fig.add_subplot(gs[0:2, :4])\n",
    "    axs.plot(data, label=arm_labels)\n",
    "    axs.set_ylabel(\"Number of times mutation was used\")\n",
    "    axs.legend()\n",
    "\n",
    "    # multiple lines all full height showing when D-TS used the dynamic update rule\n",
    "    plt.vlines(x=[i for i, e in enumerate(learner_log['update']) if e != 0],\n",
    "               ymin=0, ymax=np.max(data), colors='k', ls='-', lw=0.025)\n",
    "\n",
    "    # Plotting alphas and betas ------------------------------------------------\n",
    "    for i, col in enumerate(['UCB1']):\n",
    "        columns = learner_log.columns[learner_log.columns.str.startswith(f'{col} ')]\n",
    "        labels  = [f\"{col} {arm_labels[i]}\" for i in range(4)] \n",
    "        data    = learner_log.loc[:, columns]\n",
    "\n",
    "        axs = fig.add_subplot(gs[(i+1)*2:(i+1)*2+2, :4])\n",
    "        axs.plot(data, label=labels)\n",
    "        axs.set_ylabel(f\"{col}s\")\n",
    "        axs.legend()\n",
    "\n",
    "        # multiple lines all full height showing when D-TS used the dynamic update rule\n",
    "        axs.vlines(x=[i for i, e in enumerate(learner_log['update']) if e != 0],\n",
    "                ymin=0, ymax=np.max(data), colors='k', ls='-', lw=0.025)\n",
    "    \n",
    "    axs.set_xlabel(\"Evaluations\") # Label only on last plot\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'll create a simple bandit configuration so we can do a sanity check of our `D_MAB` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "class Bandits:\n",
    "    def __init__(self, reward_prob):\n",
    "        # Implementing simple bandits.\n",
    "        self.reward_prob = reward_prob # True reward prob., which learner shoudn't know\n",
    "        self.n_bandits   = len(reward_prob) \n",
    "\n",
    "    def pull(self, arm_idx):\n",
    "        # Sampling over a normal distr. with mu=0 and var=1\n",
    "        result = np.random.randn()\n",
    "        \n",
    "        # return a positive or nullary reward (Bernoulli random variable).\n",
    "        return 1 if result > self.reward_prob[arm_idx] else 0\n",
    "\n",
    "for probs, descr, expec in [\n",
    "    (np.array([ 1.0,  1.0, 1.0,  1.0]), 'All bandits with same probs'  , 'similar amount of pulls for each arm'         ),\n",
    "    (np.array([-1.0,  0.2, 0.0,  1.0]), 'One bandit with higher prob'  , 'more pulls for first arm, less pulls for last'),\n",
    "    (np.array([-0.2, -1.0, 0.0, -1.0]), 'Two bandits with higher probs', '2nd approx 4th > 1st > 3rd'                   ),\n",
    "]:\n",
    "    bandits = Bandits(probs)\n",
    "\n",
    "    print(\"------------------------ optimizing ------------------------\")\n",
    "\n",
    "    learner = D_MAB(4)\n",
    "    for i in range(1000):\n",
    "        arm_idx = learner.choose_arm()\n",
    "        reward  = bandits.pull(arm_idx)\n",
    "\n",
    "        learner.update(arm_idx, reward) \n",
    "\n",
    "    plot_learner_history(learner)\n",
    "    print(f\"(it was expected: {expec})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the D-MAB seems to work. Now let's add this MAB inside mutation to update PARAMS option and control dinamically the mutaiton probabilities during evolution.\n",
    "\n",
    "We can import the brush estimator and replace the `_mutation` by a custom function. Ideally, to use this python MAB optimizer, we need to have an object created to keep track of the variables, and the object needs to wrap the _pull_ action, as well as evaluating the reward based on the result.\n",
    "\n",
    "> we'll need to do a _gambiarra_ to know which mutation is used so we can correctly update `D_MAB`. All MAB logic is implemented in python, and we chose the mutation in python as well. To make sure a specific mutation was used, we force it to happen by setting others' weights to zero. this way we know exactly what happened in the C++ code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrushEstimatorMod(BrushEstimator): # Modifying brush estimator\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # mutations optimized by the learner. Learner arms correspond to\n",
    "        # these mutations in the order they appear here\n",
    "        self.mutations_ = ['point', 'insert', 'delete', 'toggle_weight']\n",
    "\n",
    "        # Whether the learner should update after each mutation, or if it should\n",
    "        # update only after a certain number of evaluations.\n",
    "        # Otherwise, it will\n",
    "        # store all rewards in gen_rewards_ (which is reseted at the beggining\n",
    "        # of every generation) and do a batch of updates only after finishing\n",
    "        # mutating the solutions.\n",
    "        self.batch_size_    = self.pop_size #\n",
    "        self.batch_rewards_ = []\n",
    "\n",
    "    def _mutate(self, ind1):\n",
    "        # Overriding the mutation so it updates our sampling method. Doing the\n",
    "        # logic on the python-side for now.\n",
    "\n",
    "        # Creating a wrapper for mutation to be able to control what is happening\n",
    "        # in the C++ code (this should be prettier in a future implementation)\n",
    "        \n",
    "        params = self.get_params()\n",
    "        \n",
    "        mutation_idx = self.learner_.choose_arm()\n",
    "\n",
    "        for i, m in enumerate(self.mutations_):\n",
    "            params['mutation_options'][m] = 0 if i != mutation_idx else 1.0\n",
    "\n",
    "        _brush.set_params(params)\n",
    "\n",
    "        opt = ind1.prg.mutate()\n",
    "\n",
    "        if opt:\n",
    "            offspring = creator.Individual(opt)\n",
    "            # print(\"mutation\")\n",
    "            # print(ind1.prg.get_model())\n",
    "            # print(offspring.prg.get_model())\n",
    "\n",
    "            offspring.fitness.values = self.toolbox_.evaluate(offspring)\n",
    "            \n",
    "            # We compare fitnesses using the deap overloaded operators\n",
    "            # from the docs: When comparing fitness values that are **minimized**,\n",
    "            # ``a > b`` will return :data:`True` if *a* is **smaller** than *b*.\n",
    "            # (this means that this comparison should work agnostic of min/max problems,\n",
    "            # or even a single-objective or multi-objective problem)\n",
    "            reward = 1.0 if offspring.fitness > ind1.fitness else 0.0\n",
    "            \n",
    "            # if not ignore_this_time:\n",
    "            #     self.batch_rewards_.append( (mutation_idx, reward) )\n",
    "\n",
    "            self.batch_rewards_.append( (mutation_idx, reward) )\n",
    "\n",
    "            if len(self.batch_rewards_) >= self.batch_size_:\n",
    "                for (mutation_idx, reward) in self.batch_rewards_:\n",
    "                    self.learner_.update(mutation_idx, reward)\n",
    "                self.batch_rewards_ = []\n",
    "            \n",
    "            return offspring\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        _brush.set_params(self.get_params())\n",
    "\n",
    "        self.data_ = self._make_data(X,y)\n",
    "        # self.data_.print()\n",
    "\n",
    "        # set n classes if relevant\n",
    "        if self.mode==\"classification\":\n",
    "            self.n_classes_ = len(np.unique(y))\n",
    "\n",
    "        # We have 4 different mutations, and the learner will learn to choose\n",
    "        # between these options by maximizing the reward when using each one\n",
    "        self.learner_ = D_MAB(4)\n",
    "\n",
    "        if isinstance(self.functions, list):\n",
    "            self.functions_ = {k:1.0 for k in self.functions}\n",
    "        else:\n",
    "            self.functions_ = self.functions\n",
    "\n",
    "        self.search_space_ = _brush.SearchSpace(self.data_, self.functions_)\n",
    "\n",
    "        self.toolbox_ = self._setup_toolbox(data=self.data_)\n",
    "\n",
    "        archive, logbook = nsga2(\n",
    "            self.toolbox_, self.max_gen, self.pop_size, 0.9, self.verbosity)\n",
    "\n",
    "        self.archive_ = archive\n",
    "        self.logbook_ = logbook\n",
    "        self.best_estimator_ = self.archive_[0].prg\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "class BrushClassifierMod(BrushEstimatorMod,ClassifierMixin):\n",
    "    def __init__( self, **kwargs):\n",
    "        super().__init__(mode='classification',**kwargs)\n",
    "\n",
    "    def _fitness_function(self, ind, data: _brush.Dataset):\n",
    "        ind.prg.fit(data)\n",
    "        return (\n",
    "            np.abs(data.y-ind.prg.predict(data)).sum(), \n",
    "            ind.prg.size()\n",
    "        )\n",
    "    \n",
    "    def _make_individual(self):\n",
    "        return creator.Individual(\n",
    "            self.search_space_.make_classifier(self.max_depth, self.max_size)\n",
    "            if self.n_classes_ == 2 else\n",
    "            self.search_space_.make_multiclass_classifier(self.max_depth, self.max_size)\n",
    "            )\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        data = self._make_data(X)\n",
    "        return self.best_estimator_.predict_proba(data)\n",
    "\n",
    "\n",
    "class BrushRegressorMod(BrushEstimatorMod, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(mode='regressor',**kwargs)\n",
    "\n",
    "    def _fitness_function(self, ind, data: _brush.Dataset):\n",
    "        ind.prg.fit(data)\n",
    "        return (\n",
    "            np.sum((data.y- ind.prg.predict(data))**2),\n",
    "            ind.prg.size()\n",
    "        )\n",
    "\n",
    "    def _make_individual(self):\n",
    "        return creator.Individual(\n",
    "            self.search_space_.make_regressor(self.max_depth, self.max_size)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to avoid racing conditions (https://deap.readthedocs.io/en/master/tutorials/basic/part4.html)\n",
    "if __name__ == '__main__':\n",
    "    from brush import BrushRegressor\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    from pmlb import fetch_data\n",
    "\n",
    "    # X, y = fetch_data('537_houses', return_X_y=True, local_cache_dir='./')\n",
    "\n",
    "    data = pd.read_csv('../../docs/examples/datasets/d_example_patients.csv')\n",
    "    X = data.drop(columns='target')\n",
    "    y = data['target']\n",
    "\n",
    "    # data = pd.read_csv('../../docs/examples/datasets/d_2x1_subtract_3x2.csv')\n",
    "    # X = data.drop(columns='target')\n",
    "    # y = data['target']\n",
    "\n",
    "    # data = pd.read_csv('../../docs/examples/datasets/d_square_x1_plus_2_x1_x2_plus_square_x2.csv')\n",
    "    # X = data.drop(columns='target')\n",
    "    # y = data['target']\n",
    "\n",
    "    kwargs = {\n",
    "        'verbosity' : False,\n",
    "        'pop_size'  : 60,\n",
    "        'max_gen'   : 300,\n",
    "        'max_depth' : 10,\n",
    "        'max_size'  : 20,\n",
    "        'mutation_options' : {\"point\":0.25, \"insert\": 0.25, \"delete\":  0.25, \"toggle_weight\": 0.25}\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "        [('Original', 'score'), ('Original', 'best model'), \n",
    "         ('Original', 'size'),  ('Original', 'depth'), ('Original', 'Time'), \n",
    "         ('Modified', 'score'), ('Modified', 'best model'), \n",
    "         ('Modified', 'size'),  ('Modified', 'depth'), ('Modified', 'Time'), \n",
    "         ('Modified', 'point mutation calls'),\n",
    "         ('Modified', 'insert mutation calls'),\n",
    "         ('Modified', 'delete mutation calls'),\n",
    "         ('Modified', 'toggle_weight mutation calls')],\n",
    "        names=('Brush version', 'metric')))\n",
    "    \n",
    "    est_mab = None\n",
    "    for i in range(30):\n",
    "        try:\n",
    "            print(f\"{i}, \", end='\\n' if (i==29) else '')\n",
    "\n",
    "            est_start_time = time.time()\n",
    "            est     = BrushRegressor(**kwargs).fit(X,y)\n",
    "            est_end_time = time.time() - est_start_time\n",
    "\n",
    "            est_mab_start_time = time.time()\n",
    "            est_mab = BrushRegressorMod(**kwargs).fit(X,y)\n",
    "            est_mab_end_time = time.time() - est_mab_start_time\n",
    "\n",
    "            learner_log = pd.DataFrame(est_mab.learner_.pull_history).set_index('t')\n",
    "            total_pulls = learner_log['arm idx'].value_counts().to_dict()\n",
    "            \n",
    "            results.loc[f'run {i}'] = [\n",
    "                # Original implementation\n",
    "                est.score(X,y), est.best_estimator_.get_model(),\n",
    "                est.best_estimator_.size(), est.best_estimator_.depth(), est_end_time,\n",
    "\n",
    "                # Implementation using Dynamic Thompson Sampling\n",
    "                est_mab.score(X,y), est_mab.best_estimator_.get_model(), \n",
    "                est_mab.best_estimator_.size(), est_mab.best_estimator_.depth(), est_mab_end_time,\n",
    "                \n",
    "                # Mutation count\n",
    "                *total_pulls.values()]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Showing results and statistics\n",
    "    display(results)\n",
    "    display(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(est_mab):\n",
    "\n",
    "    learner_log = pd.DataFrame(est_mab.learner_.pull_history).set_index('t')\n",
    "\n",
    "        # Setting up the figure layout\n",
    "    fig = plt.figure(figsize=(12, 6), tight_layout=True)\n",
    "    gs = gridspec.GridSpec(6, 6)\n",
    "\n",
    "    # Approximating the percentage of usage for each generation ----------------\n",
    "    data = np.zeros( (est_mab.max_gen, 4) )\n",
    "    for g in range(est_mab.max_gen):\n",
    "        idx_start = g*(learner_log.shape[0]//est_mab.max_gen)\n",
    "        idx_end   = (g+1)*(learner_log.shape[0]//est_mab.max_gen)\n",
    "\n",
    "        df_in_range = learner_log.iloc[idx_start:idx_end]\n",
    "        g_data = df_in_range['arm idx'].value_counts(normalize=True).to_dict()\n",
    "        for k, v in g_data.items():\n",
    "            data[g, k] = v\n",
    "\n",
    "    axs = fig.add_subplot(gs[0:3, :3])\n",
    "    axs.stackplot(range(est_mab.max_gen), data.T, labels=est_mab.mutations_)\n",
    "\n",
    "    axs.set_ylabel(\"Percentage of usage\")\n",
    "    axs.legend()\n",
    "\n",
    "    # average Brush weights for each generation --------------------------------\n",
    "    data = np.zeros( (est_mab.max_gen, 4) )\n",
    "    for g in range(est_mab.max_gen):\n",
    "        idx_start = g*(learner_log.shape[0]//est_mab.max_gen)\n",
    "        idx_end   = (g+1)*(learner_log.shape[0]//est_mab.max_gen)\n",
    "\n",
    "        learner_log_in_range = learner_log.iloc[idx_start:idx_end]\n",
    "\n",
    "        total_rewards = learner_log_in_range.groupby('arm idx')['reward'].sum().to_dict()\n",
    "        total_pulls   = learner_log_in_range['arm idx'].value_counts().to_dict()\n",
    "\n",
    "        keys = total_pulls.keys()\n",
    "        data_total_pulls    = np.array([total_pulls[k] for k in sorted(keys)])\n",
    "        data_total_rewards  = np.array([total_rewards[k] for k in sorted(keys)])\n",
    "\n",
    "        # Success rate\n",
    "        data[g, [int(i) for i in keys]] = data_total_rewards/data_total_pulls\n",
    "\n",
    "    axs = fig.add_subplot(gs[3:6, :3])\n",
    "    axs.stackplot(range(est_mab.max_gen), data.T, labels=est_mab.mutations_)\n",
    "\n",
    "    axs.set_xlabel(\"Generations\")\n",
    "    axs.set_ylabel(\"brush Weights conversion\")\n",
    "    axs.legend()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    logbook = pd.DataFrame(columns=['gen', 'evals', 'ave m1', 'ave m2',\n",
    "                                    'std m1', 'std m2', 'min m1', 'min m2'])\n",
    "    for item in est_mab.logbook_:\n",
    "        # I'll store the calculate\n",
    "        logbook.loc[item['gen']] = (\n",
    "            item['gen'], item['evals'], *item['ave'], *item['std'], *item['min']\n",
    "        )\n",
    "\n",
    "    x = logbook['gen']\n",
    "    for i, metric in enumerate(['m1', 'm2']):\n",
    "        axs = fig.add_subplot(gs[(3*i):(3*i + 3), 3:])\n",
    "\n",
    "        y     = logbook[f'ave {metric}']\n",
    "        y_err = logbook[f'std {metric}']\n",
    "        y_min = logbook[f'min {metric}']\n",
    "\n",
    "        axs.plot(x, y, 'b', label='Avg.')\n",
    "        axs.fill_between(x, y-y_err, y+y_err, fc='b', alpha=0.5, label=\"Std.\")\n",
    "        axs.plot(x, y_min, 'k', label='Min.')\n",
    "\n",
    "        axs.set_ylabel(\"Score\" if metric=='m1' else \"Size\")\n",
    "        axs.legend()\n",
    "\n",
    "    axs.set_xlabel(\"Generations\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learner_history(est_mab.learner_, arm_labels=est_mab.mutations_)\n",
    "generate_plots(est_mab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from brush import BrushClassifier\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    from pmlb import fetch_data\n",
    "\n",
    "    # X, y = fetch_data('adult', return_X_y=True, local_cache_dir='./')\n",
    "\n",
    "    data = pd.read_csv('../../docs/examples/datasets/d_analcatdata_aids.csv')\n",
    "    X = data.drop(columns='target')\n",
    "    y = data['target']\n",
    "\n",
    "    kwargs = {\n",
    "        'verbosity' : False,\n",
    "        'pop_size'  : 60,\n",
    "        'max_gen'   : 300,\n",
    "        'max_depth' : 10,\n",
    "        'max_size'  : 20,\n",
    "        'mutation_options' : {\"point\":0.25, \"insert\": 0.25, \"delete\":  0.25, \"toggle_weight\": 0.25}\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "        [('Original', 'score'), ('Original', 'best model'), \n",
    "         ('Original', 'size'),  ('Original', 'depth'), ('Original', 'Time'), \n",
    "         ('Modified', 'score'), ('Modified', 'best model'), \n",
    "         ('Modified', 'size'),  ('Modified', 'depth'), ('Modified', 'Time'), \n",
    "         ('Modified', 'point mutation calls'),\n",
    "         ('Modified', 'insert mutation calls'),\n",
    "         ('Modified', 'delete mutation calls'),\n",
    "         ('Modified', 'toggle_weight mutation calls')],\n",
    "        names=('Brush version', 'metric')))\n",
    "    \n",
    "    est_mab = None\n",
    "    for i in range(30):\n",
    "        try:\n",
    "            print(f\"{i}, \", end='\\n' if (i==29) else '')\n",
    "\n",
    "            est_start_time = time.time()\n",
    "            est = BrushClassifier(**kwargs).fit(X,y)\n",
    "            est_end_time = time.time() - est_start_time\n",
    "\n",
    "            est_mab_start_time = time.time()\n",
    "            est_mab = BrushClassifierMod(**kwargs).fit(X,y)\n",
    "            est_mab_end_time = time.time() - est_mab_start_time\n",
    "\n",
    "            learner_log = pd.DataFrame(est_mab.learner_.pull_history).set_index('t')\n",
    "            total_pulls = learner_log['arm idx'].value_counts().to_dict()\n",
    "            \n",
    "            results.loc[f'run {i}'] = [\n",
    "                # Original implementation\n",
    "                est.score(X,y), est.best_estimator_.get_model(),\n",
    "                est.best_estimator_.size(), est.best_estimator_.depth(), est_end_time,\n",
    "\n",
    "                # Implementation using Dynamic Thompson Sampling\n",
    "                est_mab.score(X,y), est_mab.best_estimator_.get_model(), \n",
    "                est_mab.best_estimator_.size(), est_mab.best_estimator_.depth(), est_mab_end_time,\n",
    "                \n",
    "                # Mutation count\n",
    "                *total_pulls.values()]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Showing results and statistics\n",
    "    display(results)\n",
    "    display(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learner_history(est_mab.learner_, arm_labels=est_mab.mutations_)\n",
    "generate_plots(est_mab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dccdbee601866cd4c45494445ca79bf9b696b8bf13c00622eb9e8a421ade3c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
