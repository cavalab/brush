{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing D-MAB, as described in DaCosta et al. - 2008 - Adaptive operator selection with dynamic multi-arm**\n",
    "\n",
    ">  (hybrid between UCB1 and Page-Hinkley (PH) test)\n",
    "\n",
    "D-MAB maintains four indicators for each arm $i$:\n",
    "1. number $n_{i, t}$ of times $i$-th arm has been played up to time $t$;\n",
    "2. the average empirical reward $\\widehat{p}_{j, t}$ at time $t$;\n",
    "3. the average and maximum deviation $m_i$ and $M_i$ involved in the PH test, initialized to $0$ and updated as detailed below. At each time step $t$:\n",
    "\n",
    "D-MAB selects the arm $i$ that maximizes equation 1:\n",
    "\n",
    "$$\\widehat{p}_{i, t} + \\sqrt{\\frac{2 \\log \\sum_{k}n_{k, t}}{n_{i, t}}}$$\n",
    "\n",
    "> Notice that the sum of the number of times each arm was pulled is equal to the time $\\sum_{k}n_{k, t} = t$, but since their algorithm resets the number of picks, we need to go with the summation. \n",
    "\n",
    "and receives some reward $r_t$, drawn after reward distribution $p_{i, t}$.\n",
    "\n",
    "> I think there is a typo in the eq. 1 on the paper. I replaced $j$ with $i$ in the lower indexes.\n",
    "\n",
    "The four indicators are updated accordingly:\n",
    "\n",
    "- $\\widehat{p}_{i, t} :=\\frac{1}{n_{i, t} + 1}(n_{i, t}\\widehat{p}_{i, t} + r_t)$\n",
    "- $n_{i, t} := n_{i, t}+1$\n",
    "- $m_i := m_i + (\\widehat{p}_{i, t} - r_t + \\delta)$\n",
    "- $M_i:= \\text{max}(M_i, m_i)$\n",
    "\n",
    "And if the PH test is triggered ($M_i - m_i > \\lambda$), the bandit is restarted, i.e., for all arms, all indicators are set to zero (the authors argue that, empirically, resetting the values is more robust than decreasing them with some mechanism such as probability matching).\n",
    "\n",
    "> I will reset to 1 instead of 0 (as the original paper does) to avoid divide by zero when calculating UCB1.\n",
    "\n",
    "The PH test is a standard test for the change hypothesis. It works by monitoring the difference between $M_i$ and $m_i$, and when the difference is greater than some uuser-specified threshold $\\lambda$, the PH test is triggered, i.e., it is considered that the Change hypothesis holds.\n",
    "\n",
    "Parameter $\\lambda$ controls the trade-off between false alarms and un-noticed changes. Parameter $\\delta$ enforces the robustness of the test when dealing with slowly varying environments.\n",
    "\n",
    "We also need a scaling mechanism to control the Exploration _versus_ Exploitation balance. They proposed two, from which I will focus on the first: Multiplicative Scaling (cUCB). **It consists on multiplying all rewards by a fixed user-defined parameter $C_{M-\\text{scale}}$.\n",
    "\n",
    "This way, we need to give to our D-MAB 3 parameters: $\\lambda$, $\\delta$, and $C_{M - \\text{scale}}$. In the paper they did a sensitivity analysis of the parameters, but I think they should be fine tuned for each specific data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_MAB:\n",
    "    def __init__(self, num_bandits, verbose=False, *, delta, lmbda, scaling, pull_f, reward_f):\n",
    "        self.num_bandits = num_bandits\n",
    "        self.verbose     = verbose\n",
    "        self.delta       = delta\n",
    "        self.lmbda       = lmbda\n",
    "        self.scaling     = scaling\n",
    "        self.pull_f      = pull_f\n",
    "        self.reward_f    = reward_f\n",
    "\n",
    "        # History of choices and time instant t (just to track the behavior)\n",
    "        self.history = {i:[] for i in range(self.num_bandits)}\n",
    "\n",
    "        self._reset_indicators()\n",
    "\n",
    "    def _reset_indicators(self):\n",
    "        self.avg_reward    = np.zeros(self.num_bandits)\n",
    "        self.num_played    = np.zeros(self.num_bandits)\n",
    "        self.avg_deviation = np.zeros(self.num_bandits)\n",
    "        self.max_deviation = np.zeros(self.num_bandits)\n",
    "\n",
    "    def _calc_UCB1s(self):\n",
    "        # log1p and +1 on denominator fixes some numeric problems in the original eq.\n",
    "        scores = np.array([self.avg_reward[i] + np.sqrt(2*np.log1p(sum(self.num_played))/(self.num_played[i]+1))\n",
    "            for i in range(self.num_bandits)])\n",
    "        \n",
    "        return np.nan_to_num(scores, nan=0)\n",
    "\n",
    "    def _scale_reward(self, reward):\n",
    "        return reward*self.scaling\n",
    "    \n",
    "    def playAndOptimize(self, *pull_args):\n",
    "        # It will pick the bandit that maximizes eq.1. \n",
    "        UCB1s  = self._calc_UCB1s()\n",
    "\n",
    "        # We need to know which arm we picked, what it returned, and how to calculate the reward given what the arm returned\n",
    "        picked = np.nanargmax(np.nan_to_num(UCB1s, nan=-np.inf))\n",
    "        pulled = self.pull_f(picked, *pull_args)\n",
    "        reward = self.reward_f(pulled)\n",
    "        \n",
    "        self.history[picked].append(reward)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Avg. Rewards: {self.avg_reward}\\nUCB1 scores : {UCB1s}\\nPicked      : {picked}\\nReward      : {reward}\")\n",
    "\n",
    "        # After choosing, it will implicitly update the parameters based on the return\n",
    "        if np.isfinite(reward):\n",
    "            self.avg_reward[picked]    = (self.num_played[picked]*self.avg_reward[picked] + self._scale_reward(reward))/(self.num_played[picked]+1)\n",
    "            self.avg_deviation[picked] = self.avg_deviation[picked] + (self.avg_reward[picked] - self._scale_reward(reward) + self.delta)\n",
    "            \n",
    "        self.num_played[picked]    = self.num_played[picked] +1\n",
    "        self.max_deviation[picked] = np.maximum(self.max_deviation[picked], self.avg_deviation[picked])\n",
    "\n",
    "        if (self.max_deviation[picked] - self.avg_deviation[picked] > self.lmbda):\n",
    "            self._reset_indicators()\n",
    "            if self.verbose:\n",
    "                print(\"Reseted indicators ----------------------------------------\")\n",
    "\n",
    "        return picked, pulled, reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'll create a simple bandit configuration so we can do a sanity check of our `D_MAB` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================\n",
      "All bandits with same probs\n",
      "------------- Uniformly Distributed Random pulls -------------\n",
      "Probabilities for each arm:  [1. 1. 1. 1.] (the smaller the better)\n",
      "cum. reward for each arm  :  [-1707, -1628, -1716, -1671]\n",
      "pulls for each arm        :  [2527, 2452, 2508, 2513]\n",
      "------------------------ optimizing ------------------------\n",
      "cum. reward for each arm:  {0: -1847, 1: -1734, 2: -1670, 3: -1559}\n",
      "pulls for each arm      :  {0: 2837, 1: 2500, 2: 2402, 3: 2261}\n",
      "(it was expected: similar amount of pulls for each arm)\n",
      "\n",
      "==============================================================\n",
      "One bandit with higher prob\n",
      "------------- Uniformly Distributed Random pulls -------------\n",
      "Probabilities for each arm:  [-1.   0.2  0.   1. ] (the smaller the better)\n",
      "cum. reward for each arm  :  [1633, -483, 80, -1686]\n",
      "pulls for each arm        :  [2435, 2547, 2518, 2500]\n",
      "------------------------ optimizing ------------------------\n",
      "cum. reward for each arm:  {0: 5127, 1: -135, 2: -21, 3: -383}\n",
      "pulls for each arm      :  {0: 7425, 1: 983, 2: 1065, 3: 527}\n",
      "(it was expected: more pulls for first arm, less pulls for last)\n",
      "\n",
      "==============================================================\n",
      "Two bandits with higher probs\n",
      "------------- Uniformly Distributed Random pulls -------------\n",
      "Probabilities for each arm:  [-0.2 -1.   0.  -1. ] (the smaller the better)\n",
      "cum. reward for each arm  :  [386, 1727, -76, 1651]\n",
      "pulls for each arm        :  [2548, 2529, 2494, 2429]\n",
      "------------------------ optimizing ------------------------\n",
      "cum. reward for each arm:  {0: 110, 1: 2769, 2: 38, 3: 2737}\n",
      "pulls for each arm      :  {0: 976, 1: 4123, 2: 864, 3: 4037}\n",
      "(it was expected: 2nd and 4th have similar number of pulls, higher than 1st and 3rd)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "import numpy as np\n",
    "\n",
    "for bandits, descr, expec in [\n",
    "    (np.array([1.0, 1.0,  1.0,  1.0]), 'All bandits with same probs', 'similar amount of pulls for each arm'),\n",
    "    (np.array([-1.0, 0.2,  0.0,  1.0]), 'One bandit with higher prob', 'more pulls for first arm, less pulls for last'),\n",
    "    (np.array([-0.2, -1.0,  0.0,  -1.0]), 'Two bandits with higher probs', '2nd and 4th have similar number of pulls, higher than 1st and 3rd'),\n",
    "]:\n",
    "    # Implementing simple bandits\n",
    "    def pullBandit(bandit):\n",
    "\n",
    "        #Get a random number based on a normal dist with mean 0 and var 1\n",
    "        result = np.random.randn()\n",
    "        \n",
    "        # bandits: This is the true reward probabilities, which we shoudn't have access (in the optimizer)\n",
    "        # return a positive or negative reward based on bandit prob.\n",
    "        return 1 if result > bandits[bandit] else -1\n",
    "\n",
    "    \n",
    "    print(\"\\n==============================================================\")\n",
    "    print(descr)\n",
    "\n",
    "    print(\"------------- Uniformly Distributed Random pulls -------------\")\n",
    "    picks   = [0, 0, 0, 0]\n",
    "    rewards = [0, 0, 0, 0]\n",
    "\n",
    "    for _ in range(10000):\n",
    "        index  = np.random.randint(len(bandits))\n",
    "        reward = pullBandit(index)\n",
    "\n",
    "        picks[index]   = picks[index]+1\n",
    "        rewards[index] = rewards[index]+reward\n",
    "\n",
    "    print(\"Probabilities for each arm: \", bandits, \"(the smaller the better)\")\n",
    "    print(\"cum. reward for each arm  : \", rewards)\n",
    "    print(\"pulls for each arm        : \", picks)\n",
    "\n",
    "    print(\"------------------------ optimizing ------------------------\")\n",
    "\n",
    "    # We have the problem that we need to determine delta and lambda values previously.\n",
    "    # This needs domain knowledge (in SR context, I think we need to know if data is homogenic or\n",
    "    # if it changes a lot through time).\n",
    "    optimizer = D_MAB(4, verbose=False, \n",
    "                      delta=0.25, lmbda=1, scaling=2,\n",
    "                      pull_f=pullBandit, reward_f=lambda r:r)\n",
    "\n",
    "    # Let's optimize\n",
    "    for i in range(10000):\n",
    "        optimizer.playAndOptimize()\n",
    "\n",
    "    total_rewards = {k : sum(v) for (k, v) in optimizer.history.items()}\n",
    "    total_played  = {k : len(v) for (k, v) in optimizer.history.items()}\n",
    "\n",
    "    print(\"cum. reward for each arm: \", total_rewards)\n",
    "    print(\"pulls for each arm      : \", total_played)\n",
    "    print(f\"(it was expected: {expec})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the D-MAB seems to work. Now let's add this MAB inside mutation to update PARAMS option and control dinamically the mutaiton probabilities during evolution.\n",
    "\n",
    "We can import the brush estimator and replace the `_mutation` by a custom function. Ideally, to use this python MAB optimizer, we need to have an object created to keep track of the variables, and the object needs to wrap the _pull_ action, as well as evaluating the reward based on the result.\n",
    "\n",
    "> we'll need to do a _gambiarra_ to know which mutation is used so we can correctly update `D_MAB`. All MAB logic is implemented in python, and we chose the mutation in python as well. To make sure a specific mutation was used, we force it to happen by setting others' weights to zero. this way we know exactly what happened in the C++ code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brush import BrushRegressor\n",
    "from deap import creator\n",
    "import _brush\n",
    "from deap_api import nsga2, DeapIndividual \n",
    "\n",
    "#prg.mutate is a convenient interface that uses the current search space to sample mutations\n",
    "\n",
    "class BrushRegressorMod(BrushRegressor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _mutate(self, ind1):\n",
    "        # Overriding the mutation so it is wrapped with D_MAB\n",
    "        \n",
    "        mutation, offspring, reward = self.D_MAB_.playAndOptimize(ind1)\n",
    "        \n",
    "        #print(mutation, ind1.prg.get_model(), offspring.prg.get_model(), reward)\n",
    "        return offspring\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        _brush.set_params(self.get_params())\n",
    "\n",
    "        self.data_ = self._make_data(X,y)\n",
    "\n",
    "        # Creating a wrapper for mutation to be able to control what is happening in the C++\n",
    "        # code (this should be prettier in a future implementation)\n",
    "        def _pull_mutation(mutation_idx, ind1):\n",
    "            mutations = ['point', 'insert', 'delete', 'toggle_weight']\n",
    "            params = self.get_params()\n",
    "\n",
    "            for i, m in enumerate(mutations):\n",
    "                params['mutation_options'][m] = 0 if i != mutation_idx else 1.0\n",
    "\n",
    "            _brush.set_params(params)\n",
    "        \n",
    "            offspring = creator.Individual(ind1.prg.mutate())\n",
    "\n",
    "            return offspring\n",
    "        \n",
    "        # Given the result of a pull (the mutated offspring), how do I evaluate it?\n",
    "        # (here I am manually writing the multi-optimization problem nsga2 is\n",
    "        # designed to solve)\n",
    "        def _evaluate_reward(ind):\n",
    "            if not ind.fitness.valid:\n",
    "                ind.prg.fit(self.data_)\n",
    "                fit = (\n",
    "                    np.sum((self.data_.y- ind.prg.predict(self.data_))**2),\n",
    "                    ind.prg.size()\n",
    "                )\n",
    "            \n",
    "                ind.fitness.values = fit\n",
    "            \n",
    "            error, size = ind.fitness.values\n",
    "            return -1.0*error + -1.0*size\n",
    "            \n",
    "        # We have 4 different mutations\n",
    "        self.D_MAB_ = D_MAB(4, verbose=False, \n",
    "                            delta=0.05, lmbda=5, scaling=1e-5, # How to determine these values???\n",
    "                            pull_f=_pull_mutation, reward_f=_evaluate_reward)\n",
    "\n",
    "        if isinstance(self.functions, list):\n",
    "            self.functions_ = {k:1.0 for k in self.functions}\n",
    "        else:\n",
    "            self.functions_ = self.functions\n",
    "\n",
    "        self.search_space_ = _brush.SearchSpace(self.data_, self.functions_)\n",
    "        self.toolbox_ = self._setup_toolbox(data=self.data_)\n",
    "\n",
    "        archive, logbook = nsga2(self.toolbox_, self.max_gen, self.pop_size, 0.9, self.verbosity)\n",
    "\n",
    "        self.archive_ = archive\n",
    "        self.best_estimator_ = self.archive_[0].prg\n",
    "        total_played  = {k : len(v) for (k, v) in self.D_MAB_.history.items()}\n",
    "\n",
    "        print(total_played)\n",
    "        print(self.D_MAB_.avg_reward)\n",
    "        print('best model:',self.best_estimator_.get_model())\n",
    "        return self\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets use this new mutation into an ES algorithm (because this is only based on mutation) and see if it improves the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------- Run 0 --------------------------------------\n",
      "{0: 2504, 1: 2440, 2: 2505, 3: 2451}\n",
      "[-0.00036238 -0.00147657 -0.00033448 -0.00126673]\n",
      "best model: 3.60*Sin(2.72*x1)\n",
      "-------------------------------------- Run 1 --------------------------------------\n",
      "{0: 2491, 1: 2480, 2: 2491, 3: 2438}\n",
      "[-0.00034092 -0.00051414 -0.0003384  -0.00126214]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 2 --------------------------------------\n",
      "{0: 63, 1: 3366, 2: 3383, 3: 3088}\n",
      "[-1.35675027e+14 -5.23720451e-04 -3.38402328e-04 -3.78375709e-03]\n",
      "best model: 2.79*Tanh(36.11*x1)\n",
      "-------------------------------------- Run 3 --------------------------------------\n",
      "{0: 3316, 1: 33, 2: 3316, 3: 3235}\n",
      "[-3.42976253e-04 -9.02122534e+06 -3.33060718e-04 -1.26440401e-03]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 4 --------------------------------------\n",
      "{0: 2491, 1: 2479, 2: 2491, 3: 2439}\n",
      "[-0.00035025 -0.00055322 -0.00033663 -0.00125379]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 5 --------------------------------------\n",
      "{0: 2532, 1: 2353, 2: 2532, 3: 2483}\n",
      "[-0.00033139 -0.00052145 -0.00033167 -0.00128344]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 6 --------------------------------------\n",
      "{0: 1006, 1: 69, 2: 4474, 3: 4351}\n",
      "[-1.19270264e+02 -7.58510521e+08 -3.38995081e-04 -1.23290815e-03]\n",
      "best model: 3.68*Sin(2.74*x1)\n",
      "-------------------------------------- Run 7 --------------------------------------\n",
      "{0: 2404, 1: 2508, 2: 2521, 3: 2467}\n",
      "[-0.00238984 -0.00054975 -0.00033945 -0.00125824]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 8 --------------------------------------\n",
      "{0: 2492, 1: 2482, 2: 2493, 3: 2433}\n",
      "[-0.00035269 -0.00053543 -0.00034014 -0.00138042]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 9 --------------------------------------\n",
      "{0: 3342, 1: 67, 2: 3272, 3: 3219}\n",
      "[-3.64264747e-04 -3.96631982e+04 -1.15092726e-03 -1.76428450e-03]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 10 --------------------------------------\n",
      "{0: 50, 1: 3287, 2: 3320, 3: 3243}\n",
      "[-3.75404346e+05 -7.05396891e-04 -3.33864937e-04 -1.20385839e-03]\n",
      "best model: 1.77*Atan(44935.85*x1)\n",
      "-------------------------------------- Run 11 --------------------------------------\n",
      "{0: 2490, 1: 2480, 2: 2491, 3: 2439}\n",
      "[-0.00036069 -0.00054273 -0.00034047 -0.00124747]\n",
      "best model: -3.68*Sin(-2.74*x1)\n",
      "-------------------------------------- Run 12 --------------------------------------\n",
      "{0: 37, 1: 3302, 2: 3321, 3: 3240}\n",
      "[-1.05164302e+01 -5.52813190e-04 -3.40930649e-04 -1.26843586e-03]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 13 --------------------------------------\n",
      "{0: 834, 1: 3028, 2: 3054, 3: 2984}\n",
      "[-2.75502973e+02 -6.63639409e-04 -3.38363262e-04 -1.24861669e-03]\n",
      "best model: 1.77*Atan(61934.75*x1)\n",
      "-------------------------------------- Run 14 --------------------------------------\n",
      "{0: 41, 1: 4907, 2: 4938, 3: 14}\n",
      "[-5.20817814e+01 -5.29582311e-04 -3.35390689e-04 -2.85428712e+00]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 15 --------------------------------------\n",
      "{0: 3, 1: 53, 2: 9725, 3: 119}\n",
      "[-4.83653914e+22 -7.22927132e+11 -3.25257909e-04 -3.57751055e+10]\n",
      "best model: Sum(-3.00*x2,1.00*x1,1.00*x1)\n",
      "-------------------------------------- Run 16 --------------------------------------\n",
      "{0: 2491, 1: 2477, 2: 2492, 3: 2440}\n",
      "[-0.0003516  -0.00059567 -0.00033944 -0.00124227]\n",
      "best model: 15.12*Log1p(-0.25*x2)\n",
      "-------------------------------------- Run 17 --------------------------------------\n",
      "{0: 2296, 1: 2544, 2: 2557, 3: 2503}\n",
      "[-0.00502172 -0.00055057 -0.00033935 -0.00125745]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 18 --------------------------------------\n",
      "{0: 2269, 1: 2990, 2: 2309, 3: 2332}\n",
      "[-0.00037545 -0.00054674 -0.00035153 -0.00117992]\n",
      "best model: Sum(-3.00*x2,2.00*x1)\n",
      "-------------------------------------- Run 19 --------------------------------------\n",
      "{0: 10, 1: 2, 2: 9827, 3: 61}\n",
      "[-7.06470496e+00 -4.92061890e+02 -3.33184955e-04 -8.38450786e+19]\n",
      "best model: Sub(Sub(-3.00*x2,-2.00*x1),-0.00)\n",
      "-------------------------------------- Run 20 --------------------------------------\n",
      "{0: 82, 1: 3287, 2: 3306, 3: 3225}\n",
      "[-9.27254828e+08 -5.59666960e-04 -3.36175699e-04 -1.26693352e-03]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 21 --------------------------------------\n",
      "{0: 13, 1: 129, 2: 4953, 3: 4805}\n",
      "[-3.01053856e+00 -4.48513422e+01 -3.35925360e-04 -1.27177044e-03]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 22 --------------------------------------\n",
      "{0: 4864, 1: 130, 2: 4856, 3: 50}\n",
      "[-3.37258326e-04 -7.82234841e-01 -3.86614190e-04 -7.52158401e+00]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 23 --------------------------------------\n",
      "{0: 20, 1: 4913, 2: 4960, 3: 7}\n",
      "[-4.45937630e+08 -6.21522756e-04 -3.37090806e-04 -3.78629295e+00]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 24 --------------------------------------\n",
      "{0: 2491, 1: 2479, 2: 2491, 3: 2439}\n",
      "[-0.00033773 -0.00054754 -0.00033348 -0.00125877]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 25 --------------------------------------\n",
      "{0: 511, 1: 63, 2: 4730, 3: 4596}\n",
      "[-2.34806575e+02 -1.14142934e+03 -3.31660989e-04 -1.23667294e-03]\n",
      "best model: 3.60*Sin(2.72*x1)\n",
      "-------------------------------------- Run 26 --------------------------------------\n",
      "{0: 261, 1: 3202, 2: 3256, 3: 3181}\n",
      "[-1.39243889e+03 -5.36742485e-04 -3.30236506e-04 -1.21460595e-03]\n",
      "best model: 2.79*Tanh(469.33*x1)\n",
      "-------------------------------------- Run 27 --------------------------------------\n",
      "{0: 3208, 1: 97, 2: 3338, 3: 3257}\n",
      "[-1.83823120e-03 -8.56630083e+01 -3.46224261e-04 -1.26335784e-03]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 28 --------------------------------------\n",
      "{0: 3250, 1: 107, 2: 3217, 3: 3326}\n",
      "[-4.41842511e-04 -9.93119541e-01 -3.29223757e-04 -1.27331961e-03]\n",
      "best model: -4.24*x2\n",
      "-------------------------------------- Run 29 --------------------------------------\n",
      "{0: 2491, 1: 2480, 2: 2491, 3: 2438}\n",
      "[-0.00034242 -0.00051649 -0.00033197 -0.00126607]\n",
      "best model: -4.24*x2\n",
      "Score (30 runs): 0.7299758445645649\n",
      "gen\tevals\tave          \tstd                    \tmin      \n",
      "0  \t100  \t[  nan 20.98]\t[       nan 1.05811153]\t[nan 20.]\n",
      "1  \t0    \t[ nan 16.1]  \t[      nan 5.9084685]  \t[nan  1.]\n",
      "2  \t0    \t[ nan 8.84]  \t[       nan 5.75972222]\t[nan  1.]\n",
      "3  \t0    \t[ nan 2.99]  \t[       nan 2.31730447]\t[nan  1.]\n",
      "4  \t0    \t[ nan 1.28]  \t[       nan 0.56709788]\t[nan  1.]\n",
      "5  \t0    \t[26.92582146  1.02      ]\t[10.60409505  0.14      ]\t[17.82939148  1.        ]\n",
      "6  \t0    \t[22.88155502  1.01      ]\t[4.47787129 0.09949874]  \t[17.82939148  1.        ]\n",
      "7  \t0    \t[21.88920412  1.01      ]\t[4.48789957 0.09949874]  \t[17.82939148  1.        ]\n",
      "8  \t0    \t[20.44578463  1.01      ]\t[4.09343184 0.09949874]  \t[17.82939148  1.        ]\n",
      "9  \t0    \t[19.18279257  1.01      ]\t[3.2211926  0.09949874]  \t[17.82939148  1.        ]\n",
      "10 \t0    \t[18.00981892  1.        ]\t[1.26299206 0.        ]  \t[17.82939148  1.        ]\n",
      "11 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "12 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "13 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "14 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "15 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "16 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "17 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "18 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "19 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "20 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "21 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "22 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "23 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "24 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "25 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "26 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "27 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "28 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "29 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "30 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "31 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "32 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "33 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "34 \t0    \t[17.65109756  1.02      ]\t[1.77400205 0.19899749]  \t[4.01456646e-13 1.00000000e+00]\n",
      "35 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "36 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "37 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "38 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "39 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "40 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "41 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "42 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "43 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "44 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "45 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "46 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "47 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "48 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "49 \t0    \t[17.65109756  1.02      ]\t[1.77400205 0.19899749]  \t[4.01456646e-13 1.00000000e+00]\n",
      "50 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "51 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "52 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "53 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "54 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "55 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "56 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "57 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "58 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "59 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "60 \t0    \t[17.65109757  1.02      ]\t[1.77400204 0.19899749]  \t[1.35152462e-07 1.00000000e+00]\n",
      "61 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "62 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "63 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "64 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "65 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "66 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "67 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "68 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "69 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "70 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "71 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "72 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "73 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "74 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "75 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "76 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "77 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "78 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "79 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "80 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "81 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "82 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "83 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "84 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "85 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "86 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "87 \t0    \t[17.47280365  1.04      ]\t[2.49611479 0.28      ]  \t[1.35152462e-07 1.00000000e+00]\n",
      "88 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "89 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "90 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "91 \t0    \t[17.65109756  1.02      ]\t[1.77400205 0.19899749]  \t[1.59872116e-13 1.00000000e+00]\n",
      "92 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "93 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "94 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "95 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "96 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "97 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "98 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "99 \t0    \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]      \n",
      "Final population hypervolume is 48126.359818\n",
      "{0: 2502, 1: 2444, 2: 2503, 3: 2451}\n",
      "[-0.00033761 -0.00053674 -0.00033033 -0.00127082]\n",
      "best model: -4.24*x2\n",
      "score: 0.651326594086648\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# I am getting tons of unharmful warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#df = pd.read_csv('../../docs/examples/datasets/d_enc.csv')\n",
    "#X = df.drop(columns='label')\n",
    "#y = df['label']\n",
    "\n",
    "df = pd.read_csv('../../docs/examples/datasets/d_2x1_subtract_3x2.csv')\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "kwargs = {\n",
    "    'pop_size'  : 100,\n",
    "    'max_gen'   : 100,\n",
    "    'verbosity' : 0,\n",
    "    'max_depth' : 10,\n",
    "    'max_size'  : 20,\n",
    "    'mutation_options' : {\"point\":0.25, \"insert\": 0.25, \"delete\":  0.25, \"toggle_weight\": 0.25}\n",
    "}\n",
    "\n",
    "# 30 executions just to compare avg score\n",
    "scores = []\n",
    "for i in range(30):\n",
    "    print(f\"-------------------------------------- Run {i} --------------------------------------\")\n",
    "    est_mab = BrushRegressorMod(**kwargs)\n",
    "\n",
    "    # use like you would a sklearn regressor\n",
    "    est_mab.fit(X,y)\n",
    "    y_pred = est_mab.predict(X)\n",
    "\n",
    "    scores.append(est_mab.score(X,y))\n",
    "print(f\"Score (30 runs): {np.mean(scores)}\")\n",
    "\n",
    "# Single run with verbosity\n",
    "kwargs['verbosity'] = 1\n",
    "est_mab = BrushRegressorMod(**kwargs)\n",
    "\n",
    "# use like you would a sklearn regressor\n",
    "est_mab.fit(X,y)\n",
    "y_pred = est_mab.predict(X)\n",
    "\n",
    "print('score:', est_mab.score(X,y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with the original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3366, 1: 16, 2: 3370, 3: 3148}\n",
      "[-4.05962183e-04 -1.12526941e+11 -3.62192692e-04 -2.91524196e-03]\n",
      "best model: -4.24*x2\n",
      "{0: 2490, 1: 2479, 2: 2491, 3: 2440}\n",
      "[-0.00036652 -0.00054139 -0.00033253 -0.00123083]\n",
      "best model: 3.60*Sin(2.72*x1)\n",
      "{0: 3305, 1: 63, 2: 3305, 3: 3227}\n",
      "[-3.47628793e-04 -4.52236312e+00 -3.38324719e-04 -1.24529618e-03]\n",
      "best model: 3.60*Sin(2.72*x1)\n",
      "{0: 84, 1: 3286, 2: 3305, 3: 3225}\n",
      "[-3.19134285e+08 -5.59272536e-04 -3.41379892e-04 -1.25437165e-03]\n",
      "best model: 3.60*Sin(2.72*x1)\n",
      "{0: 19, 1: 107, 2: 110, 3: 9664}\n",
      "[-6.07635009e+01 -5.12827890e+17 -8.54437202e+09 -9.15424529e-04]\n",
      "best model: 3.68*Sin(2.74*x1)\n",
      "{0: 25, 1: 76, 2: 4974, 3: 4825}\n",
      "[-2.77204121e+00 -9.65579745e-01 -3.84279937e-04 -1.32060016e-03]\n",
      "best model: -4.24*x2\n",
      "{0: 129, 1: 4868, 2: 4897, 3: 6}\n",
      "[-3.16229731e-01 -5.37142383e-04 -3.59548331e-04 -3.51616980e+17]\n",
      "best model: -4.24*x2\n",
      "{0: 1537, 1: 2799, 2: 2812, 3: 2752}\n",
      "[-0.03085949 -0.00054441 -0.00033255 -0.0012708 ]\n",
      "best model: 2.79*Tanh(469.33*x1)\n",
      "{0: 45, 1: 3300, 2: 3318, 3: 3237}\n",
      "[-5.85096838e+00 -5.39978020e-04 -3.38116597e-04 -1.25995611e-03]\n",
      "best model: -4.24*x2\n",
      "{0: 4885, 1: 59, 2: 4886, 3: 70}\n",
      "[-3.34650998e-04 -7.58935695e+19 -3.33819759e-04 -3.53269682e+00]\n",
      "best model: -4.24*x2\n",
      "{0: 2403, 1: 2383, 2: 2585, 3: 2529}\n",
      "[-0.003463   -0.00383799 -0.00033757 -0.0012668 ]\n",
      "best model: 3.60*Sin(2.72*x1)\n",
      "{0: 119, 1: 3098, 2: 3385, 3: 3298}\n",
      "[-8.54277606e-01 -3.70314404e-03 -3.63485341e-04 -1.33930528e-03]\n",
      "best model: 1.26*Floor(-3.00*x2)\n",
      "{0: 242, 1: 3191, 2: 3293, 3: 3174}\n",
      "[-5.10587726e+02 -5.11324171e-04 -3.27858040e-04 -1.23800979e-03]\n",
      "best model: 1.77*Atan(44757.84*x1)\n",
      "{0: 2485, 1: 2482, 2: 2493, 3: 2440}\n",
      "[-0.00047499 -0.0005332  -0.000337   -0.00126153]\n",
      "best model: -4.24*x2\n",
      "{0: 64, 1: 3238, 2: 3339, 3: 3259}\n",
      "[-8.88350612e+02 -1.47782443e-03 -3.37196647e-04 -1.24099308e-03]\n",
      "best model: 1.77*Atan(94709.32*x1)\n",
      "{0: 2491, 1: 2479, 2: 2491, 3: 2439}\n",
      "[-0.00034939 -0.0005673  -0.00034729 -0.00125779]\n",
      "best model: -4.24*x2\n",
      "{0: 2490, 1: 2479, 2: 2491, 3: 2440}\n",
      "[-0.00036089 -0.00055806 -0.00034671 -0.0012465 ]\n",
      "best model: 16.53*Log1p(-0.23*x2)\n",
      "{0: 85, 1: 54, 2: 4955, 3: 4806}\n",
      "[-4.33770528e+02 -2.90762476e+08 -3.36727177e-04 -1.27426962e-03]\n",
      "best model: -4.24*x2\n",
      "{0: 2498, 1: 2438, 2: 2509, 3: 2455}\n",
      "[-0.00053392 -0.0015817  -0.00033776 -0.0012685 ]\n",
      "best model: -4.24*x2\n",
      "{0: 328, 1: 3205, 2: 3222, 3: 3145}\n",
      "[-6.22900813e+02 -5.37273429e-04 -3.34606208e-04 -1.25118665e-03]\n",
      "best model: 1.77*Atan(58403.29*x1)\n",
      "{0: 32, 1: 3304, 2: 3323, 3: 3241}\n",
      "[-1.11523542e+01 -5.45819514e-04 -3.28464819e-04 -1.27437150e-03]\n",
      "best model: -4.24*x2\n",
      "{0: 2, 1: 115, 2: 9775, 3: 8}\n",
      "[-2.76785925e+01 -2.27590186e+05 -3.35505547e-04 -2.24268908e+02]\n",
      "best model: -4.24*x2\n",
      "{0: 70, 1: 3331, 2: 3269, 3: 3230}\n",
      "[-1.43217558e+03 -5.27008308e-04 -3.26690376e-04 -1.24314573e-03]\n",
      "best model: 2.79*Tanh(42.92*x1)\n",
      "{0: 239, 1: 1750, 2: 93, 3: 7818}\n",
      "[-5.02028184e+02 -8.83763100e+08 -5.62765928e+08 -1.10354608e-03]\n",
      "best model: 1.77*Atan(118371.02*x1)\n",
      "{0: 924, 1: 3009, 2: 3009, 3: 2958}\n",
      "[-1.29857556e+02 -5.76118924e-04 -5.77231624e-04 -1.25136432e-03]\n",
      "best model: 1.77*Atan(44757.84*x1)\n",
      "{0: 190, 1: 47, 2: 52, 3: 9611}\n",
      "[-7.10204387e+01 -1.64681556e+01 -1.00648539e+08 -1.03053944e-03]\n",
      "best model: 1.77*Atan(215807.25*x1)\n",
      "{0: 2049, 1: 2, 2: 3977, 3: 3872}\n",
      "[-2.70667978e-02 -6.33814783e+08 -3.33925210e-04 -1.25067235e-03]\n",
      "best model: 2.79*Tanh(469.33*x1)\n",
      "{0: 25, 1: 4926, 2: 59, 3: 4890}\n",
      "[-1.14996572e+01 -8.30932822e-04 -5.01314766e+06 -1.04938353e-03]\n",
      "best model: 1.26*Floor(-3.00*x2)\n",
      "{0: 4, 1: 3313, 2: 3332, 3: 3251}\n",
      "[-2.76152882e+01 -5.53825483e-04 -3.39124161e-04 -1.25374732e-03]\n",
      "best model: -4.24*x2\n",
      "{0: 125, 1: 4868, 2: 4895, 3: 12}\n",
      "[-2.95068578e+00 -5.26443489e-04 -3.62352455e-04 -1.35562820e+14]\n",
      "best model: -4.24*x2\n",
      "Score (30 runs): 0.7256975662463311\n",
      "gen\tevals\tave          \tstd                    \tmin      \n",
      "0  \t100  \t[  nan 20.83]\t[       nan 1.00054985]\t[nan 20.]\n",
      "1  \t100  \t[ nan 12.2]  \t[      nan 7.4939976]  \t[nan  1.]\n",
      "2  \t100  \t[ nan 3.42]  \t[       nan 3.09896757]\t[nan  1.]\n",
      "3  \t100  \t[28.58473202  1.        ]\t[12.62085635  0.        ]\t[17.82939148  1.        ]\n",
      "4  \t100  \t[19.81409328  1.        ]\t[3.73706994 0.        ]  \t[17.82939148  1.        ]\n",
      "5  \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "6  \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "7  \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "8  \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "9  \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "10 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "11 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "12 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "13 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "14 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "15 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "16 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "17 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "18 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "19 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "20 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "21 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "22 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "23 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "24 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "25 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "26 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "27 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "28 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "29 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "30 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "31 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "32 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "33 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "34 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "35 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "36 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "37 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "38 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "39 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "40 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "41 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "42 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "43 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "44 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "45 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "46 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "47 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "48 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "49 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "50 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "51 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "52 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "53 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "54 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "55 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "56 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "57 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "58 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "59 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "60 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "61 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "62 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "63 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "64 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "65 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "66 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "67 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "68 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "69 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "70 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "71 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "72 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "73 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "74 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "75 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "76 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "77 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "78 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "79 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "80 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "81 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "82 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "83 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "84 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "85 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "86 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "87 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "88 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "89 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "90 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "91 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "92 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "93 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "94 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "95 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "96 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "97 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "98 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "99 \t100  \t[17.82939148  1.        ]\t[0. 0.]                  \t[17.82939148  1.        ]\n",
      "Final population hypervolume is 48126.359818\n",
      "best model: -4.24*x2\n",
      "score: 0.651326594086648\n"
     ]
    }
   ],
   "source": [
    "# 30 executions just to compare avg score\n",
    "scores = []\n",
    "for _ in range(30):\n",
    "    kwargs['verbosity'] = 0\n",
    "\n",
    "    est_mab = BrushRegressorMod(**kwargs)\n",
    "\n",
    "    # use like you would a sklearn regressor\n",
    "    est_mab.fit(X,y)\n",
    "    y_pred = est_mab.predict(X)\n",
    "\n",
    "    scores.append(est_mab.score(X,y))\n",
    "print(f\"Score (30 runs): {np.mean(scores)}\")\n",
    "\n",
    "# Single run with verbosity\n",
    "\n",
    "kwargs['verbosity'] = 1\n",
    "est = BrushRegressor(**kwargs)\n",
    "\n",
    "# use like you would a sklearn regressor\n",
    "est.fit(X,y)\n",
    "y_pred = est.predict(X)\n",
    "\n",
    "print('score:', est.score(X,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
