{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Dynamic) Thompson Sampling, as described in Gupta et al. - 2011 - Thompson sampling for dynamic multi-armed bandits**\n",
    "\n",
    "> Thompson sampling is a probabilistic approach to solve the Multi-Armed Bandit. This paper modifies the original algorithm to make it handle distribution changes during the execution\n",
    "\n",
    "The Thompson Sampling in this paper considers that each arm is a Bernoulli trial, having the output set ${0, 1}$, with $\\theta^k$ denoting the probability of success for arm $k$.\n",
    "\n",
    "The probability distribution of successes $S$ obtained in $n^k$ trials is a Binomial distribution:\n",
    "\n",
    "$$p(S = s|\\theta^k) = \\binom{n^k}{s} (1-\\theta^k)^{n-s}(\\theta^k)^s.$$\n",
    "\n",
    "The Beta distribution is a conjugate prior (is of the same probability distribution family as the prior probability, which is the Binomial distribution), parameterized by $\\alpha_0$ and $\\beta_0$:\n",
    "\n",
    "$$p(\\widehat{\\theta}^k; \\alpha_0, \\beta_0) = \\frac{x^{\\alpha_0-1}(1-x)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)},$$\n",
    "\n",
    "with $B$ being a binonial distribution.\n",
    "\n",
    "> We use conjugate prior to derive a closed-form expression for the posterior distribution, usually easier to interpret, manipulate and update. In Bayesian statistics, we adjust the hyperparameters of the posterior distribution to optimize the likelihood with the prior distribution.\n",
    "\n",
    "The **original Thompson sampling** updates $\\alpha_n$ and $\\beta_n$ for the $n$-th trial, with reward $r_n$ as:\n",
    "\n",
    "$$\\alpha^k_ n = \\alpha^k_{n-1} + r_n,$$\n",
    "$$\\beta^k_ n = \\beta^k_{n-1} + (1-r_n).$$\n",
    "\n",
    "The proposed method extends the original algorithm by inserting a new update rule based on an hyperparameter $C$. $C$ is a threshold that provides exponential weighting of the outcomes of the trials, making more recent rewards getting more weight. This way, if prior distributions change during the execution, the learned posterior distributions would respond to it.\n",
    "\n",
    "We update $\\alpha_n$ and $\\beta_n$ conditionally based on $C$:\n",
    "\n",
    "If $\\alpha_{n-1}+\\beta_{n-1}<C$, we use the original Thompson Sampling. Otherwise, we update\n",
    "\n",
    "$$\\alpha^k_ n = (\\alpha^k_{n-1} + r_n)\\frac{C}{C+1},$$\n",
    "$$\\beta^k_ n = (\\beta^k_{n-1} + (1-r_n))\\frac{C}{C+1}.$$\n",
    "\n",
    "> The paper suggest initializing all $\\alpha$ and $\\beta$ with the value $2$ for all arms.\n",
    "\n",
    "The remaining of the paper performs an sensitivity analysis and some experiments to check how well the Dynamic Thompson Sampling performs.\n",
    "\n",
    "> In our work, the mutations would be the arms, and this update would be used during the evolution to adjust the mutation probabilities.\n",
    "\n",
    "> Brush originally sample the mutations using an uniform distribution. This algorithm learns hyperparameters to Beta distributions. Somehow we need to convert them to have a transparent implementation to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class D_TS:\n",
    "    def __init__(self, num_bandits, C=100):\n",
    "        self.num_bandits = num_bandits\n",
    "\n",
    "        # Store learner status when the update function is called\n",
    "        self.pull_history = {\n",
    "            c:[] for c in ['t', 'arm idx', 'reward', 'update'] + \n",
    "                          [f'alpha {i}'  for i in range(num_bandits)] + \n",
    "                          [f'beta {i}'   for i in range(num_bandits)] + \n",
    "                          [f'weight {i}' for i in range(num_bandits)] } \n",
    "\n",
    "        # This is the probability that should be used to update brush probs\n",
    "        self._probabilities = np.ones(num_bandits)/num_bandits\n",
    "\n",
    "        self._alphas = 2*np.ones(num_bandits) # Paper suggests starting with 2's\n",
    "        self._betas  = 2*np.ones(num_bandits)\n",
    "        self.C       = C # how to define this value???\n",
    "\n",
    "    @property\n",
    "    def probabilities(self):\n",
    "        return self._probabilities\n",
    "    \n",
    "    @probabilities.setter\n",
    "    def probabilities(self, new_probabilities):\n",
    "        if len(self._probabilities)==len(new_probabilities):\n",
    "            self._probabilities = new_probabilities\n",
    "        else:\n",
    "            print(f\"New probabilities must have size {self.num_bandits}\")\n",
    "\n",
    "    def choose_arm(self):\n",
    "        \"\"\"Uses the learned distributions to randomly choose an arm to pull. \n",
    "        \n",
    "        Returns the index of the arm that was choosen based on the Beta\n",
    "        probabilities of previous successes and fails.\n",
    "        \"\"\"\n",
    "        \n",
    "        # probability estimates from the beta distribution\n",
    "        thetas = np.random.beta(self._alphas, self._betas)\n",
    "        \n",
    "        arm_idx = np.argmax(thetas)\n",
    "        \n",
    "        return arm_idx\n",
    "    \n",
    "    def update(self, arm_idx, reward):\n",
    "        # There are informations about state. we'll save the pull history of\n",
    "        # other stuff after updating their values\n",
    "        self.pull_history['t'].append( len(self.pull_history['t']) )\n",
    "        self.pull_history['arm idx'].append( arm_idx )\n",
    "        self.pull_history['reward'].append( reward )\n",
    "        \n",
    "        if self._alphas[arm_idx] + self._betas[arm_idx] < self.C:\n",
    "            # This is the pure thompson scheme\n",
    "            self._alphas[arm_idx] = self._alphas[arm_idx]+reward\n",
    "            self._betas[arm_idx]  = self._betas[arm_idx]+(1-reward)\n",
    "\n",
    "            self.pull_history['update'].append( 0 )\n",
    "        else:\n",
    "            # This is the dynamic adjust\n",
    "            self._alphas[arm_idx] = (self._alphas[arm_idx]+reward)*(self.C/(self.C+1))\n",
    "            self._betas[arm_idx]  = (self._betas[arm_idx]+(1-reward))*(self.C/(self.C+1))\n",
    "\n",
    "            self.pull_history['update'].append( 1 )\n",
    "\n",
    "        # How to transform our Beta distributions into node probabilities?\n",
    "        # onde idea is to return the expected value of this distribution as\n",
    "        # the weight that will be given to each arm. In the case of our prior\n",
    "        # (which is a beta distribution), the expected value is given by\n",
    "        # 1 / (1 + beta/alpha)\n",
    "        self._probabilities = 1 / (1 + (self._betas/self._alphas))\n",
    "\n",
    "        # Now that we finished updating the values we save them to the logs\n",
    "        for i in range(self.num_bandits):\n",
    "            self.pull_history[f'alpha {i}'].append( self._alphas[i] )\n",
    "            self.pull_history[f'beta {i}'].append( self._betas[i] )\n",
    "            self.pull_history[f'weight {i}'].append( self.probabilities[i] )\n",
    "\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "import pandas as pd\n",
    "\n",
    "class Bandits:\n",
    "    def __init__(self, reward_prob):\n",
    "        # Implementing simple bandits.\n",
    "        self.reward_prob = reward_prob # True reward prob., which learner shoudn't know\n",
    "        self.n_bandits   = len(reward_prob) \n",
    "\n",
    "    def pull(self, arm_idx):\n",
    "        # Sampling over a normal distr. with mu=0 and var=1\n",
    "        result = np.random.randn()\n",
    "        \n",
    "        # return a positive or nullary reward (Bernoulli random variable).\n",
    "        return 1 if result > self.reward_prob[arm_idx] else 0\n",
    "\n",
    "for probs, descr, expec in [\n",
    "    (np.array([ 1.0,  1.0, 1.0,  1.0]), 'All bandits with same probs'  , 'similar amount of pulls for each arm'         ),\n",
    "    (np.array([-1.0,  0.2, 0.0,  1.0]), 'One bandit with higher prob'  , 'more pulls for first arm, less pulls for last'),\n",
    "    (np.array([-0.2, -1.0, 0.0, -1.0]), 'Two bandits with higher probs', '2nd approx 4th > 1st > 3rd'                   ),\n",
    "]:\n",
    "    bandits = Bandits(probs)\n",
    "\n",
    "    print(\"------------------------ optimizing ------------------------\")\n",
    "\n",
    "    learner = D_TS(4)\n",
    "    for i in range(10000):\n",
    "        arm_idx = learner.choose_arm()\n",
    "        reward  = bandits.pull(arm_idx)\n",
    "\n",
    "        learner.update(arm_idx, reward) \n",
    "\n",
    "    learner_log = pd.DataFrame(learner.pull_history).set_index('t')\n",
    "\n",
    "    total_rewards = learner_log.groupby('arm idx')['reward'].sum().to_dict()\n",
    "    total_pulls   = learner_log['arm idx'].value_counts().to_dict()\n",
    "\n",
    "    print(\"cum. reward for each arm    : \", total_rewards)\n",
    "    print(\"number of pulls for each arm: \", total_pulls)\n",
    "    print(f\"(it was expected: {expec})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brush.estimator import BrushEstimator\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "from deap import creator\n",
    "import _brush\n",
    "from deap_api import nsga2 \n",
    "\n",
    "class BrushEstimatorMod(BrushEstimator): # Modifying brush estimator\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # mutations optimized by the learner. Learner arms correspond to\n",
    "        # these mutations in the order they appear here\n",
    "        self.mutations_ = ['point', 'insert', 'delete', 'toggle_weight']\n",
    "\n",
    "        # Whether the learner should update after each mutation, or if it should\n",
    "        # update only after a certain number of evaluations.\n",
    "        # Otherwise, it will\n",
    "        # store all rewards in gen_rewards_ (which is reseted at the beggining\n",
    "        # of every generation) and do a batch of updates only after finishing\n",
    "        # mutating the solutions.\n",
    "        self.batch_size_    = self.pop_size #\n",
    "        self.batch_rewards_ = []\n",
    "\n",
    "    def _mutate(self, ind1):\n",
    "        # Overriding the mutation so it updates our sampling method. Doing the\n",
    "        # logic on the python-side for now.\n",
    "\n",
    "        # Creating a wrapper for mutation to be able to control what is happening\n",
    "        # in the C++ code (this should be prettier in a future implementation)\n",
    "        \n",
    "        params = self.get_params()\n",
    "        \n",
    "        # if the mutation returns an invalid expression, this should count as reward=0\n",
    "        # ignore_this_time = True if (ind1.prg.size()+1>=self.max_size\n",
    "        #                             or ind1.prg.depth()+1>=self.max_depth) else False\n",
    "\n",
    "        # Insert Mutation will not work, even if we force it, when the expression\n",
    "        # is already at maximum size.\n",
    "        # In this case, we'll do the mutation without controlling the probabilities.\n",
    "        # if ignore_this_time:\n",
    "        #     for i, m in enumerate(self.mutations_):\n",
    "        #         params['mutation_options'][m] = 0.25 # let cpp do the mutation \n",
    "        # else:\n",
    "            # mutation_idx = self.learner_.choose_arm()\n",
    "\n",
    "            # for i, m in enumerate(self.mutations_):\n",
    "            #     params['mutation_options'][m] = 0 if i != mutation_idx else 1.0\n",
    "\n",
    "        mutation_idx = self.learner_.choose_arm()\n",
    "\n",
    "        for i, m in enumerate(self.mutations_):\n",
    "            params['mutation_options'][m] = 0 if i != mutation_idx else 1.0\n",
    "\n",
    "        _brush.set_params(params)\n",
    "    \n",
    "        opt = ind1.prg.mutate()\n",
    "\n",
    "        if opt:\n",
    "            offspring = creator.Individual(opt)\n",
    "            # print(\"mutation\")\n",
    "            # print(ind1.prg.get_model())\n",
    "            # print(offspring.prg.get_model())\n",
    "\n",
    "            offspring.fitness.values = self.toolbox_.evaluate(offspring)\n",
    "            \n",
    "            # We compare fitnesses using the deap overloaded operators\n",
    "            # from the docs: When comparing fitness values that are **minimized**,\n",
    "            # ``a > b`` will return :data:`True` if *a* is **smaller** than *b*.\n",
    "            # (this means that this comparison should work agnostic of min/max problems,\n",
    "            # or even a single-objective or multi-objective problem)\n",
    "            reward = 1.0 if offspring.fitness > ind1.fitness else 0.0\n",
    "            \n",
    "            # if not ignore_this_time:\n",
    "            #     self.batch_rewards_.append( (mutation_idx, reward) )\n",
    "\n",
    "            self.batch_rewards_.append( (mutation_idx, reward) )\n",
    "\n",
    "            if len(self.batch_rewards_) >= self.batch_size_:\n",
    "                for (mutation_idx, reward) in self.batch_rewards_:\n",
    "                    self.learner_.update(mutation_idx, reward)\n",
    "                self.batch_rewards_ = []\n",
    "            \n",
    "            return offspring\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        _brush.set_params(self.get_params())\n",
    "\n",
    "        self.data_ = self._make_data(X,y)\n",
    "        # self.data_.print()\n",
    "\n",
    "        # set n classes if relevant\n",
    "        if self.mode==\"classification\":\n",
    "            self.n_classes_ = len(np.unique(y))\n",
    "\n",
    "        # We have 4 different mutations, and the learner will learn to choose\n",
    "        # between these options by maximizing the reward when using each one\n",
    "        self.learner_ = D_TS(4, C=self.pop_size) # C=self.pop_size\n",
    "\n",
    "        if isinstance(self.functions, list):\n",
    "            self.functions_ = {k:1.0 for k in self.functions}\n",
    "        else:\n",
    "            self.functions_ = self.functions\n",
    "\n",
    "        self.search_space_ = _brush.SearchSpace(self.data_, self.functions_)\n",
    "\n",
    "        self.toolbox_ = self._setup_toolbox(data=self.data_)\n",
    "\n",
    "        archive, logbook = nsga2(\n",
    "            self.toolbox_, self.max_gen, self.pop_size, 0.9, self.verbosity)\n",
    "\n",
    "        self.archive_ = archive\n",
    "        self.logbook_ = logbook\n",
    "        self.best_estimator_ = self.archive_[0].prg\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "class BrushClassifierMod(BrushEstimatorMod,ClassifierMixin):\n",
    "    def __init__( self, **kwargs):\n",
    "        super().__init__(mode='classification',**kwargs)\n",
    "\n",
    "    def _fitness_function(self, ind, data: _brush.Dataset):\n",
    "        ind.prg.fit(data)\n",
    "        return (\n",
    "            np.abs(data.y-ind.prg.predict(data)).sum(), \n",
    "            ind.prg.size()\n",
    "        )\n",
    "    \n",
    "    def _make_individual(self):\n",
    "        return creator.Individual(\n",
    "            self.search_space_.make_classifier(self.max_depth, self.max_size)\n",
    "            if self.n_classes_ == 2 else\n",
    "            self.search_space_.make_multiclass_classifier(self.max_depth, self.max_size)\n",
    "        )\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        data = self._make_data(X)\n",
    "        return self.best_estimator_.predict_proba(data)\n",
    "\n",
    "\n",
    "class BrushRegressorMod(BrushEstimatorMod, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(mode='regressor',**kwargs)\n",
    "\n",
    "    def _fitness_function(self, ind, data: _brush.Dataset):\n",
    "        ind.prg.fit(data)\n",
    "        return (\n",
    "            np.sum((data.y- ind.prg.predict(data))**2),\n",
    "            ind.prg.size()\n",
    "        )\n",
    "\n",
    "    def _make_individual(self):\n",
    "        return creator.Individual(\n",
    "            self.search_space_.make_regressor(self.max_depth, self.max_size)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to avoid racing conditions (https://deap.readthedocs.io/en/master/tutorials/basic/part4.html)\n",
    "if __name__ == '__main__':\n",
    "    from brush import BrushRegressor\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # data = pd.read_csv('../../docs/examples/datasets/d_enc.csv')\n",
    "    # X = data.drop(columns='label')\n",
    "    # y = data['label']\n",
    "\n",
    "    # data = pd.read_csv('../../docs/examples/datasets/d_2x1_subtract_3x2.csv')\n",
    "    # X = data.drop(columns='target')\n",
    "    # y = data['target']\n",
    "\n",
    "    data = pd.read_csv('../../docs/examples/datasets/d_square_x1_plus_2_x1_x2_plus_square_x2.csv')\n",
    "    X = data.drop(columns='target')\n",
    "    y = data['target']\n",
    "\n",
    "    kwargs = {\n",
    "        'verbosity' : True,\n",
    "        'pop_size'  : 100,\n",
    "        'max_gen'   : 100,\n",
    "        'max_depth' : 10,\n",
    "        'max_size'  : 20,\n",
    "        'mutation_options' : {\"point\":0.25, \"insert\": 0.25, \"delete\":  0.25, \"toggle_weight\": 0.25}\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "        [('Original', 'score'), ('Original', 'best model'), \n",
    "         ('Original', 'size'),  ('Original', 'depth'), \n",
    "         ('Modified', 'score'), ('Modified', 'best model'), \n",
    "         ('Modified', 'size'),  ('Modified', 'depth'), \n",
    "         ('Modified', 'point mutation calls'),\n",
    "         ('Modified', 'insert mutation calls'),\n",
    "         ('Modified', 'delete mutation calls'),\n",
    "         ('Modified', 'toggle_weight mutation calls')],\n",
    "        names=('Brush version', 'metric')))\n",
    "    \n",
    "    est_mab = None\n",
    "    for i in range(30):\n",
    "        try:\n",
    "            print(f\"{i}, \", end='\\n' if (i==29) else '')\n",
    "\n",
    "            print(f\"est, \", end='\\n' if (i==29) else '')\n",
    "            est     = BrushRegressor(**kwargs).fit(X,y)\n",
    "            print(f\"fit, \", end='\\n' if (i==29) else '')\n",
    "            est.score(X,y)\n",
    "\n",
    "            print(f\"est_mab, \", end='\\n' if (i==29) else '')\n",
    "            est_mab = BrushRegressorMod(**kwargs).fit(X,y)\n",
    "            print(f\"fit, \", end='\\n' if (i==29) else '')\n",
    "            est_mab.score(X,y)\n",
    "\n",
    "            learner_log = pd.DataFrame(est_mab.learner_.pull_history).set_index('t')\n",
    "                        \n",
    "            total_rewards = learner_log.groupby('arm idx')['reward'].sum().to_dict()\n",
    "            total_pulls   = learner_log['arm idx'].value_counts().to_dict()\n",
    "            \n",
    "            results.loc[f'run {i}'] = [\n",
    "                # Original implementation\n",
    "                est.score(X,y), est.best_estimator_.get_model(),\n",
    "                est.best_estimator_.size(), est.best_estimator_.depth(),\n",
    "\n",
    "                # Implementation using Dynamic Thompson Sampling\n",
    "                est_mab.score(X,y), est_mab.best_estimator_.get_model(), \n",
    "                est_mab.best_estimator_.size(), est_mab.best_estimator_.depth(),\n",
    "                \n",
    "                # Mutation count\n",
    "                *total_pulls.values()]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Showing results and statistics\n",
    "    display(results)\n",
    "    display(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(est.best_estimator_.get_model())\n",
    "\n",
    "# mut = est.best_estimator_.mutate()\n",
    "# if mut:\n",
    "#     print(est.best_estimator_.get_model())\n",
    "#     print(mut.get_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots():\n",
    "    !pip install matplotlib > /dev/null\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # plot the cumulative number of pulls (for evaluations, not generations) ---\n",
    "    data = np.zeros( (learner_log.shape[0]+1, 4) )\n",
    "    for i, row in learner_log.iterrows():\n",
    "        data[i+1, :] = data[i]\n",
    "        data[i+1, row['arm idx'].astype(int)] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.plot(data, label=est_mab.mutations_)\n",
    "    plt.xlabel(\"Evaluations\")\n",
    "    plt.ylabel(\"Number of times mutation was used\")\n",
    "\n",
    "    # multiple lines all full height showing when D-TS used the dynamic update rule\n",
    "    plt.vlines(x=[i for i, e in enumerate(learner_log['update']) if e != 0],\n",
    "               ymin=0, ymax=np.max(data), colors='k', ls='-', lw=0.025)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    for i, col in enumerate(['alpha', 'beta']):\n",
    "        columns = learner_log.columns[learner_log.columns.str.startswith(f'{col} ')]\n",
    "        labels  = [columns[i].replace(str(i), est_mab.mutations_[i]) for i in range(4)] \n",
    "        data    = learner_log.loc[:, columns]\n",
    "\n",
    "        axs[i].plot(data, label=labels)\n",
    "        axs[i].set_xlabel(\"Evaluations\")\n",
    "        axs[i].set_ylabel(f\"{col}s\")\n",
    "        axs[i].legend()\n",
    "\n",
    "        # multiple lines all full height showing when D-TS used the dynamic update rule\n",
    "        axs[i].vlines(x=[i for i, e in enumerate(learner_log['update']) if e != 0],\n",
    "                ymin=0, ymax=np.max(data), colors='k', ls='-', lw=0.025)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Approximating the percentage of usage for each generation ----------------\n",
    "    # TODO: test if different batch sizes will produce different plots here\n",
    "    data = np.zeros( (kwargs['max_gen'], 4) )\n",
    "    for g in range(kwargs['max_gen']):\n",
    "        idx_start = g*(learner_log.shape[0]//kwargs['max_gen'])\n",
    "        idx_end   = (g+1)*(learner_log.shape[0]//kwargs['max_gen'])\n",
    "\n",
    "        df_in_range = learner_log.iloc[idx_start:idx_end]\n",
    "        g_data = df_in_range['arm idx'].value_counts(normalize=True).to_dict()\n",
    "        for k, v in g_data.items():\n",
    "            data[g, k] = v\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    #plt.plot(data, label=est_mab.mutations_)\n",
    "    plt.stackplot(range(kwargs['max_gen']), data.T, labels=est_mab.mutations_)\n",
    "    plt.xlabel(\"Generations\")\n",
    "    plt.ylabel(\"Percentage of usage\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    logbook = pd.DataFrame(columns=['gen', 'evals', 'ave m1', 'ave m2',\n",
    "                                    'std m1', 'std m2', 'min m1', 'min m2'])\n",
    "    for item in est_mab.logbook_:\n",
    "        # I'll store the calculate\n",
    "        logbook.loc[item['gen']] = (\n",
    "            item['gen'], item['evals'], *item['ave'], *item['std'], *item['min']\n",
    "        )\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    x = logbook['gen']\n",
    "    for i, metric in enumerate(['m1', 'm2']):\n",
    "        y     = logbook[f'ave {metric}']\n",
    "        y_err = logbook[f'std {metric}']\n",
    "        y_min = logbook[f'min {metric}']\n",
    "\n",
    "        axs[i].plot(x, y, 'b', label='Avg.')\n",
    "        axs[i].fill_between(x, y-y_err, y+y_err, fc='b', alpha=0.5, label=\"Std.\")\n",
    "        axs[i].plot(x, y_min, 'k', label='Min.')\n",
    "\n",
    "        axs[i].set_xlabel(\"Generation\")\n",
    "        axs[i].set_ylabel(\"Score\" if metric=='m1' else \"Size\")\n",
    "        axs[i].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "generate_plots()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from brush import BrushClassifier\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    from pmlb import fetch_data\n",
    "\n",
    "    # X, y = fetch_data('adult', return_X_y=True, local_cache_dir='./')\n",
    "\n",
    "    data = pd.read_csv('../../docs/examples/datasets/d_analcatdata_aids.csv')\n",
    "    X = data.drop(columns='target')\n",
    "    y = data['target']\n",
    "\n",
    "    kwargs = {\n",
    "        'verbosity' : False,\n",
    "        'pop_size'  : 100,\n",
    "        'max_gen'   : 100,\n",
    "        'max_depth' : 10,\n",
    "        'max_size'  : 20,\n",
    "        'mutation_options' : {\"point\":0.25, \"insert\": 0.25, \"delete\":  0.25, \"toggle_weight\": 0.25}\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "        [('Original', 'score'), ('Original', 'best model'), \n",
    "         ('Original', 'size'),  ('Original', 'depth'), \n",
    "         ('Modified', 'score'), ('Modified', 'best model'), \n",
    "         ('Modified', 'size'),  ('Modified', 'depth'), \n",
    "         ('Modified', 'point mutation calls'),\n",
    "         ('Modified', 'insert mutation calls'),\n",
    "         ('Modified', 'delete mutation calls'),\n",
    "         ('Modified', 'toggle_weight mutation calls')],\n",
    "        names=('Brush version', 'metric')))\n",
    "    \n",
    "    est_mab = None\n",
    "    for i in range(30):\n",
    "        try:\n",
    "            print(f\"{i}, \", end='\\n' if (i==29) else '')\n",
    "\n",
    "            est = BrushClassifier(**kwargs).fit(X,y)\n",
    "            est_mab = BrushClassifierMod(**kwargs).fit(X,y)\n",
    "\n",
    "            learner_log = pd.DataFrame(est_mab.learner_.pull_history).set_index('t')\n",
    "                        \n",
    "            total_rewards = learner_log.groupby('arm idx')['reward'].sum().to_dict()\n",
    "            total_pulls   = learner_log['arm idx'].value_counts().to_dict()\n",
    "            \n",
    "            results.loc[f'run {i}'] = [\n",
    "                # Original implementation\n",
    "                est.score(X,y), est.best_estimator_.get_model(),\n",
    "                est.best_estimator_.size(), est.best_estimator_.depth(),\n",
    "\n",
    "                # Implementation using Dynamic Thompson Sampling\n",
    "                est_mab.score(X,y), est_mab.best_estimator_.get_model(), \n",
    "                est_mab.best_estimator_.size(), est_mab.best_estimator_.depth(),\n",
    "                \n",
    "                # Mutation count\n",
    "                *total_pulls.values()]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Showing results and statistics\n",
    "    display(results)\n",
    "    display(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
